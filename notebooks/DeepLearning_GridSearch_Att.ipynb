{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "#import glob\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.models import densenet121, vgg16, resnet50, inception_v3, resnet18\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from common.myfunctions import plot_confusion_matrix\n",
    "from common.customloss import QuadraticKappa, WeightedMultiLabelLogLoss\n",
    "from common.blocks import ConvBlock, LinearAttentionBlock, ProjectorBlock\n",
    "from common.nets import ResNet50Attention\n",
    "from common.densenet import densenet121att\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/srv/app/data'\n",
    "\n",
    "DATA_DIR = BASE_DIR + '/data'\n",
    "\n",
    "MODEL_DIR = BASE_DIR + '/models'\n",
    "\n",
    "TRAIN_DIR = DATA_DIR + '/numpy_array/stage_1_train_images_299'\n",
    "TEST_DIR = DATA_DIR + '/numpy_array/stage_1_train_images_299' #Same path because we split train in train and test.\n",
    "\n",
    "TRAIN_LABELS = DATA_DIR + '/stage_1_train_pivoted.csv'\n",
    "TEST_LABELS = DATA_DIR + ''\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "NUM_EPOCH = 30\n",
    "\n",
    "TEST_SPLIT = 0.3\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "NUM_CLASSES = 6\n",
    "\n",
    "LABEL_COLUMN = ['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
    "\n",
    "INPUT_SIZE = 299\n",
    "\n",
    "IMAGE_FORMAT = 'npy'\n",
    "\n",
    "MODELS = ['FineTuningResNet50Attention']\n",
    "\n",
    "OPTIMIZERS = ['DefaultAdam']\n",
    "\n",
    "LOSSES = ['WeightedMultiLabelLogLoss']\n",
    "\n",
    "#DATASETS = ['WLWW_4040_50100_6040']\n",
    "\n",
    "SAMPLE_FRAC = 0.01 #Fraction of dataset to use. Set to 1.0 to use the entire dataset.\n",
    "\n",
    "CUDA_DEVICES = [2,3]\n",
    "\n",
    "BLACK_LIST_ID = ['ID_6431af929', 'ID_8da38f2e4', 'ID_0e21abf7a', 'ID_470e639ae', 'ID_d91d52bdc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LIST = {\n",
    "    'PreDensenet121': {\n",
    "        'base_model': 'densenet121',\n",
    "        'is_inception': False,\n",
    "        'pretrained': True,\n",
    "        'load_checkpoint': False,\n",
    "        'checkpoint_from': '',\n",
    "        'layers_to_frozen': ['conv0', 'norm0', 'denseblock1', 'transition1', \n",
    "                             'denseblock2', 'transition2', 'denseblock3', 'transition3', \n",
    "                             'denseblock4', 'norm5']\n",
    "    },\n",
    "    'FineTuningDensenet121': {\n",
    "        'base_model': 'densenet121',\n",
    "        'is_inception': False,\n",
    "        'pretrained': True,\n",
    "        'load_checkpoint': False,\n",
    "        'checkpoint_from': '',\n",
    "        'layers_to_frozen': []\n",
    "    },\n",
    "    'FineTuningDensenet121v1': {\n",
    "        'base_model': 'densenet121',\n",
    "        'is_inception': False,\n",
    "        'pretrained': True,\n",
    "        'load_checkpoint': False,\n",
    "        'checkpoint_from': '',\n",
    "        'layers_to_frozen': ['conv0', 'norm0', 'denseblock1', 'transition1', \n",
    "                             'denseblock2', 'transition2', 'denseblock3', 'transition3']\n",
    "    },\n",
    "    'FineTuningDensenet121v2': {\n",
    "        'base_model': 'densenet121',\n",
    "        'is_inception': False,\n",
    "        'pretrained': True,\n",
    "        'load_checkpoint': True,\n",
    "        'checkpoint_from': 'FineTuningDensenet121v1',\n",
    "        'layers_to_frozen': ['conv0', 'norm0', 'denseblock1', 'transition1', \n",
    "                             'denseblock2', 'transition2']\n",
    "    },\n",
    "    'PreVGG16': {\n",
    "        'base_model': 'vgg16',\n",
    "        'is_inception': False,\n",
    "        'pretrained': True,\n",
    "        'load_checkpoint': False,\n",
    "        'checkpoint_from': '',\n",
    "        'layers_to_frozen': ['features.0', 'features.2', 'features.5', 'features.7', \n",
    "                             'features.10', 'features.12', 'features.14', 'features.17', \n",
    "                             'features.19', 'features.21', 'features.24', 'features.26', \n",
    "                             'features.28', 'classifier.0', 'classifier.3']\n",
    "    },\n",
    "    'PreResNet50': {\n",
    "        'base_model': 'resnet50',\n",
    "        'is_inception': False,\n",
    "        'pretrained': True,\n",
    "        'load_checkpoint': False,\n",
    "        'checkpoint_from': '',\n",
    "        'layers_to_frozen': ['conv1', 'bn1', 'layer1', 'layer2', 'layer3', 'layer4']\n",
    "    },\n",
    "    'FineTuningResNet50Attention': {\n",
    "        'base_model': 'ResNet50Attention',\n",
    "        'is_inception': False,\n",
    "        'pretrained': True,\n",
    "        'load_checkpoint': False,\n",
    "        'checkpoint_from': '',\n",
    "        'layers_to_frozen': []\n",
    "    },\n",
    "    'FineTuningResNet50': {\n",
    "        'base_model': 'resnet50',\n",
    "        'is_inception': False,\n",
    "        'pretrained': True,\n",
    "        'load_checkpoint': False,\n",
    "        'checkpoint_from': '',\n",
    "        'layers_to_frozen': []\n",
    "    },\n",
    "    'FineTuningResNet50v1': {\n",
    "        'base_model': 'resnet50',\n",
    "        'is_inception': False,\n",
    "        'pretrained': True,\n",
    "        'load_checkpoint': False,\n",
    "        'checkpoint_from': '',\n",
    "        'layers_to_frozen': ['conv1', 'bn1', 'layer1', 'layer2', 'layer3']\n",
    "    },\n",
    "    'FineTuningResNet50v2': {\n",
    "        'base_model': 'resnet50',\n",
    "        'is_inception': False,\n",
    "        'pretrained': True,\n",
    "        'load_checkpoint': True,\n",
    "        'checkpoint_from': 'FineTuningResNet50v1',\n",
    "        'layers_to_frozen': ['conv1', 'bn1', 'layer1', 'layer2']\n",
    "    },\n",
    "    'PreInceptionV3': {\n",
    "        'base_model': 'inception_v3',\n",
    "        'is_inception': True,\n",
    "        'pretrained': True,\n",
    "        'load_checkpoint': False,\n",
    "        'checkpoint_from': '',\n",
    "        'layers_to_frozen': ['AuxLogits.conv0', 'AuxLogits.conv1', 'Conv2d_1a_3x3', 'Conv2d_2a_3x3', \n",
    "                             'Conv2d_2b_3x3', 'Conv2d_3b_1x1', 'Conv2d_4a_3x3', \n",
    "                             'Mixed_5b.branch1x1', 'Mixed_5b.branch3x3dbl_1', \n",
    "                             'Mixed_5b.branch3x3dbl_2', 'Mixed_5b.branch3x3dbl_3', 'Mixed_5b.branch5x5_1', \n",
    "                             'Mixed_5b.branch5x5_2', 'Mixed_5b.branch_pool', 'Mixed_5c.branch1x1', \n",
    "                             'Mixed_5c.branch3x3dbl_1', 'Mixed_5c.branch3x3dbl_2', 'Mixed_5c.branch3x3dbl_3', \n",
    "                             'Mixed_5c.branch5x5_1', 'Mixed_5c.branch5x5_2', 'Mixed_5c.branch_pool', \n",
    "                             'Mixed_5d.branch1x1', 'Mixed_5d.branch3x3dbl_1', 'Mixed_5d.branch3x3dbl_2', \n",
    "                             'Mixed_5d.branch3x3dbl_3', 'Mixed_5d.branch5x5_1', 'Mixed_5d.branch5x5_2', \n",
    "                             'Mixed_5d.branch_pool', 'Mixed_6a.branch3x3', 'Mixed_6a.branch3x3dbl_1', \n",
    "                             'Mixed_6a.branch3x3dbl_2', 'Mixed_6a.branch3x3dbl_3', 'Mixed_6b.branch1x1', \n",
    "                             'Mixed_6b.branch7x7_1', 'Mixed_6b.branch7x7_2', 'Mixed_6b.branch7x7_3', \n",
    "                             'Mixed_6b.branch7x7dbl_1', 'Mixed_6b.branch7x7dbl_2', 'Mixed_6b.branch7x7dbl_3', \n",
    "                             'Mixed_6b.branch7x7dbl_4', 'Mixed_6b.branch7x7dbl_5', 'Mixed_6b.branch_pool', \n",
    "                             'Mixed_6c.branch1x1', 'Mixed_6c.branch7x7_1', 'Mixed_6c.branch7x7_2', \n",
    "                             'Mixed_6c.branch7x7_3', 'Mixed_6c.branch7x7dbl_1', 'Mixed_6c.branch7x7dbl_2', \n",
    "                             'Mixed_6c.branch7x7dbl_3', 'Mixed_6c.branch7x7dbl_4', 'Mixed_6c.branch7x7dbl_5', \n",
    "                             'Mixed_6c.branch_pool', 'Mixed_6d.branch1x1', 'Mixed_6d.branch7x7_1', \n",
    "                             'Mixed_6d.branch7x7_2', 'Mixed_6d.branch7x7_3', 'Mixed_6d.branch7x7dbl_1', \n",
    "                             'Mixed_6d.branch7x7dbl_2', 'Mixed_6d.branch7x7dbl_3', 'Mixed_6d.branch7x7dbl_4', \n",
    "                             'Mixed_6d.branch7x7dbl_5', 'Mixed_6d.branch_pool', 'Mixed_6e.branch1x1', \n",
    "                             'Mixed_6e.branch7x7_1', 'Mixed_6e.branch7x7_2', 'Mixed_6e.branch7x7_3', \n",
    "                             'Mixed_6e.branch7x7dbl_1', 'Mixed_6e.branch7x7dbl_2', 'Mixed_6e.branch7x7dbl_3', \n",
    "                             'Mixed_6e.branch7x7dbl_4', 'Mixed_6e.branch7x7dbl_5', 'Mixed_6e.branch_pool', \n",
    "                             'Mixed_7a.branch3x3_1', 'Mixed_7a.branch3x3_2', 'Mixed_7a.branch7x7x3_1', \n",
    "                             'Mixed_7a.branch7x7x3_2', 'Mixed_7a.branch7x7x3_3', 'Mixed_7a.branch7x7x3_4', \n",
    "                             'Mixed_7b.branch1x1', 'Mixed_7b.branch3x3_1', 'Mixed_7b.branch3x3_2a', \n",
    "                             'Mixed_7b.branch3x3_2b', 'Mixed_7b.branch3x3dbl_1', 'Mixed_7b.branch3x3dbl_2', \n",
    "                             'Mixed_7b.branch3x3dbl_3a', 'Mixed_7b.branch3x3dbl_3b', 'Mixed_7b.branch_pool', \n",
    "                             'Mixed_7c.branch1x1', 'Mixed_7c.branch3x3_1', 'Mixed_7c.branch3x3_2a', \n",
    "                             'Mixed_7c.branch3x3_2b', 'Mixed_7c.branch3x3dbl_1', 'Mixed_7c.branch3x3dbl_2', \n",
    "                             'Mixed_7c.branch3x3dbl_3a', 'Mixed_7c.branch3x3dbl_3b', 'Mixed_7c.branch_pool']\n",
    "    },\n",
    "    'PreEfficientNetB7': {\n",
    "        'base_model': 'efficientnetb7',\n",
    "        'is_inception': False,\n",
    "        'pretrained': True,\n",
    "        'load_checkpoint': False,\n",
    "        'checkpoint_from': '',\n",
    "        'layers_to_frozen': ['_conv_stem', '_bn0', '_blocks', '_conv_head', '_bn1']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER_LIST = {\n",
    "    'DefaultAdam': {\n",
    "        'function': 'Adam',\n",
    "        'lr': 0.001,\n",
    "        'betas': (0.9, 0.999),\n",
    "        'eps': 1e-08,\n",
    "        'weight_decay': 0,\n",
    "        'amsgrad': False\n",
    "    }\n",
    "}\n",
    "\n",
    "LOSS_LIST = {\n",
    "    'DefaultNLLLoss': {\n",
    "        'function': 'NLLLoss',\n",
    "        'weight': None,\n",
    "        'size_average': None,\n",
    "        'ignore_index': -100,\n",
    "        'reduce': None,\n",
    "        'reduction': 'mean'\n",
    "    },\n",
    "    'DefaultSmoothL1Loss': {\n",
    "        'function': 'SmoothL1Loss',\n",
    "        'size_average': None,\n",
    "        'reduce': None,\n",
    "        'reduction': 'mean'\n",
    "    },\n",
    "    'DefaultCrossEntropyLoss': {\n",
    "        'function': 'CrossEntropyLoss',\n",
    "        'weight': None,\n",
    "        'size_average': None,\n",
    "        'ignore_index': -100,\n",
    "        'reduce': None,\n",
    "        'reduction': 'mean'\n",
    "    },\n",
    "    'QuadraticKappa': {\n",
    "        'function': 'QuadraticKappa',\n",
    "        'n_classes': NUM_CLASSES\n",
    "    },\n",
    "    'WeightedMultiLabelLogLoss': {\n",
    "        'function': 'WeightedMultiLabelLogLoss',\n",
    "        'n_classes': NUM_CLASSES,\n",
    "        'weight': None\n",
    "    }\n",
    "}\n",
    "\n",
    "#DATASET_LIST = {\n",
    "#    'WLWW_4040_50100_6040': {\n",
    "#        'convert_BGR2RGB': False,\n",
    "#        'normalize_255': False,\n",
    "#        'channels': 3,\n",
    "#        'channels_first': True,\n",
    "#        'custom_function': None\n",
    "#    },\n",
    "#    'WLWW_4040': {\n",
    "#        'train_dir': DATA_DIR + '/numpy_array/stage_1_train_images_299',\n",
    "#        'val_dir': None,\n",
    "#        'test_dir': DATA_DIR + '/numpy_array/stage_1_train_images_299'\n",
    "#        'convert_BGR2RGB': False,\n",
    "#        'normalize_255': False,\n",
    "#        'channels': 3,\n",
    "#        'channels_first': True,\n",
    "#        'custom_function': None\n",
    "#    }\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "is_cuda=False\n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True\n",
    "print(is_cuda)    \n",
    "\n",
    "# Detect if we have a GPU available\n",
    "cuda_list = ','.join(str(c) for c in CUDA_DEVICES)\n",
    "device = torch.device(\"cuda:{}\".format(cuda_list) if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y, img_folder, img_ext='png', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (dataframe): Dataframe with images ID.\n",
    "            y (dataframe): Dataframe with labels annotations.\n",
    "            img_folder (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.img_folder = img_folder\n",
    "        self.img_ext = img_ext\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_folder, self.X.iloc[idx, 0] + '.' + self.img_ext)\n",
    "        \n",
    "       #image = np.load(img_name).astype('uint8')\n",
    "        image = np.load(img_name)\n",
    "        \n",
    "        label = self.y.iloc[idx].to_numpy()\n",
    "        \n",
    "        if self.transform:\n",
    "       \n",
    "           image = self.transform(TF.to_pil_image(image))\n",
    "\n",
    "        return (image,label)\n",
    "    \n",
    "# class CustomDataset_v2(Dataset):\n",
    "\n",
    "#     def __init__(self, X, y, img_folder, img_ext='png', transform=None):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             X (dataframe): Dataframe with images ID.\n",
    "#             y (dataframe): Dataframe with labels annotations.\n",
    "#             img_folder (string): Directory with all the images.\n",
    "#             transform (callable, optional): Optional transform to be applied\n",
    "#                 on a sample.\n",
    "#         \"\"\"\n",
    "#         self.X = X\n",
    "#         self.y = y\n",
    "#         self.img_folder = img_folder\n",
    "#         self.img_ext = img_ext\n",
    "#         self.transform = transform\n",
    "#         self.hu_min = -1024 # Min value of Hounsfield scale\n",
    "#         self.hu_max = 3071 # Max value of Hounsfield scale\n",
    "#         self.hu_delta = self.hu_max - self.hu_min # Just to save calculation\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.y)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_name = os.path.join(self.img_folder, self.X.iloc[idx, 0] + '.' + self.img_ext)\n",
    "#         #image = np.load(img_name).astype('uint8')\n",
    "#         image = np.load(img_name)\n",
    "        \n",
    "#         image = image[:,:,[2]]\n",
    "#         image = np.repeat(image, 3, axis=2)\n",
    "        \n",
    "#         label = self.y.iloc[idx].to_numpy()\n",
    "        \n",
    "#         if self.transform:\n",
    "        \n",
    "#             image = self.transform(TF.to_pil_image(image))\n",
    "\n",
    "#         return (image,label)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Calc Classes Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(TRAIN_LABELS)\n",
    "data = data.loc[~data.id.isin(BLACK_LIST_ID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f74052d2358>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[LABEL_COLUMN].sum().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAMPLE_FRAC < 1.0:\n",
    "    data = data.sample(frac = SAMPLE_FRAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f73f971ec18>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[LABEL_COLUMN].sum().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calc Classes Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.39719454, 11.20833333,  1.13144059,  1.7495935 ,  1.1350211 ,\n",
       "        0.94635004])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distrib_freq = data[LABEL_COLUMN].sum().to_numpy()\n",
    "w_classes = distrib_freq.sum() / (NUM_CLASSES * distrib_freq)\n",
    "w_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Wrong Loss Function Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in LOSSES:\n",
    "    if 'weight' in LOSS_LIST[l]:\n",
    "        LOSS_LIST[l]['weight'] = torch.from_numpy(w_classes).to(device)\n",
    "    else:\n",
    "        raise Exception('You are trying to set weight in a loss function without weight parameter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop(LABEL_COLUMN, axis=1)\n",
    "y = data[LABEL_COLUMN]\n",
    "\n",
    "# Criando o dataframe de treine e teste com base no dataframe anteriormente criado\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = TEST_SPLIT, random_state = SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transf = transforms.Compose([\n",
    "    transforms.RandomRotation((0,360)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transf = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(X=X_train, \n",
    "                              y=y_train, \n",
    "                              img_folder=TRAIN_DIR,\n",
    "                              img_ext=IMAGE_FORMAT, \n",
    "                              transform=train_transf)\n",
    "\n",
    "test_dataset = CustomDataset(X=X_test, \n",
    "                             y=y_test, \n",
    "                             img_folder=TEST_DIR, \n",
    "                             img_ext=IMAGE_FORMAT,\n",
    "                             transform=test_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garregando os dados\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_dict = {'train': train_loader, 'val': test_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class ResNet50Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, attention=True, normalize_attn=True):\n",
    "        super(ResNet50Attention, self).__init__()\n",
    "        self.attention = attention\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        resnet50_model = resnet50(pretrained=True)\n",
    "        \n",
    "        self.layers = [l for l in resnet50_model.children()]\n",
    "        \n",
    "        self.conv1 = layers[0]\n",
    "        self.bn1 = layers[1]\n",
    "        self.relu = layers[2]\n",
    "        self.maxpool = layers[3]\n",
    "\n",
    "        self.layer1 = layers[4]\n",
    "        self.layer2 = layers[5]\n",
    "        self.layer3 = layers[6]\n",
    "        self.layer4 = layers[7]\n",
    "\n",
    "        self.avgpool = layers[8]\n",
    "\n",
    "        if self.attention:\n",
    "            self.fc = nn.Linear(in_features=1792, out_features=self.num_classes, bias=True)\n",
    "        else:\n",
    "            self.fc = nn.Linear(in_features=2048, out_features=self.num_classes, bias=True)\n",
    "        \n",
    "        if self.attention:\n",
    "            self.projector1 = ProjectorBlock(2048, 256)\n",
    "            self.projector2 = ProjectorBlock(2048, 512)\n",
    "            self.projector3 = ProjectorBlock(2048, 1024)\n",
    "            self.attn1 = LinearAttentionBlock(in_features=256, normalize_attn=normalize_attn)\n",
    "            self.attn2 = LinearAttentionBlock(in_features=512, normalize_attn=normalize_attn)\n",
    "            self.attn3 = LinearAttentionBlock(in_features=1024, normalize_attn=normalize_attn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        l1 = self.layer1(x)\n",
    "        l2 = self.layer2(l1)\n",
    "        l3 = self.layer3(l2)\n",
    "        x = self.layer4(l3)\n",
    "        g = self.avgpool(x)\n",
    "        #x = torch.flatten(g, 1)\n",
    "        \n",
    "        # pay attention\n",
    "        if self.attention:\n",
    "            p1 = self.projector1(g)\n",
    "            c1, g1 = self.attn1(l1, p1)\n",
    "            \n",
    "            p2 = self.projector2(g)\n",
    "            c2, g2 = self.attn2(l2, p2)\n",
    "            \n",
    "            p3 = self.projector3(g)\n",
    "            c3, g3 = self.attn3(l3, p3)\n",
    "            \n",
    "            g = torch.cat((g1,g2,g3), dim=1) # batch_sizexC\n",
    "            # classification layer\n",
    "            x = self.fc(g) # batch_sizexnum_classes\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            x = self.fc(g)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2372,  0.0490, -0.3935, -0.0903,  0.0495, -0.3144]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = densenet121att(pretrained=True)\n",
    "model.classifier = nn.Linear(1024, NUM_CLASSES)\n",
    "outputs = model(inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBestModel(model_name):\n",
    "    device = torch.device(\"cuda:{}\".format(cuda_list) if torch.cuda.is_available() else \"cpu\")\n",
    "    # Get lastest model file\n",
    "    list_of_files = glob.glob(MODEL_DIR + f'/*{model_name}*.pt') # * means all if need specific format then *.csv\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    print(f'Loading model {latest_file}')\n",
    "    model = torch.load(latest_file, map_location=device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(model_name, num_classes):\n",
    "    \n",
    "    model_parameters = MODEL_LIST[model_name]\n",
    "    \n",
    "    if model_parameters['load_checkpoint']:\n",
    "        \n",
    "        model = loadBestModel(model_parameters['checkpoint_from'])\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "                    \n",
    "    else:\n",
    "        \n",
    "        if model_parameters['base_model']=='densenet121':\n",
    "            \n",
    "            model = densenet121(pretrained=model_parameters['pretrained'])\n",
    "            model.classifier = nn.Linear(1024, num_classes)   \n",
    "            \n",
    "        if model_parameters['base_model']=='vgg16':\n",
    "            \n",
    "            model = vgg16(pretrained=model_parameters['pretrained'])\n",
    "            model.classifier[6] = nn.Linear(4096, num_classes) \n",
    "        \n",
    "        if model_parameters['base_model']=='resnet50':\n",
    "            \n",
    "            model = resnet50(pretrained=model_parameters['pretrained'])\n",
    "            model.fc = nn.Linear(2048, num_classes) \n",
    "        \n",
    "        if model_parameters['base_model']=='ResNet50Attention':\n",
    "            model = ResNet50Attention(num_classes, attention=True, pretrained=model_parameters['pretrained'])\n",
    "         \n",
    "        if model_parameters['base_model']=='inception_v3':\n",
    "            \n",
    "            model = inception_v3(pretrained=model_parameters['pretrained'])\n",
    "            model.fc = nn.Linear(2048, num_classes) \n",
    "            model.AuxLogits.fc = nn.Linear(768, num_classes)\n",
    "            \n",
    "        elif model_parameters['base_model']=='efficientnetb7':\n",
    "            \n",
    "            model = EfficientNet.from_pretrained('efficientnet-b7')\n",
    "            model._fc = nn.Linear(2560, NUM_CLASSES) \n",
    "            \n",
    "        if (torch.cuda.device_count() > 1) & (not model_parameters['is_inception']):\n",
    "            if not CUDA_DEVICES:\n",
    "                print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "                model = nn.DataParallel(model) # When load checkpoint, the DataParallel is already in the model.\n",
    "            else:\n",
    "                print(\"Let's use\", CUDA_DEVICES, \"GPUs!\")\n",
    "                model = nn.DataParallel(model, device_ids = CUDA_DEVICES) # When load checkpoint, the DataParallel is already in the model.\n",
    "        \n",
    "    for name, param in model.named_parameters():\n",
    "        for l in model_parameters['layers_to_frozen']:\n",
    "            if l in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    if is_cuda:\n",
    "        model = model.to(device)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOptimizer(optimizer_name, model):\n",
    "\n",
    "    params_to_update = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "    \n",
    "        if param.requires_grad == True:\n",
    "        \n",
    "            params_to_update.append(param)\n",
    "            \n",
    "            print(\"\\t\",name)\n",
    "\n",
    "    opt_parameters = OPTIMIZER_LIST[optimizer_name]\n",
    "\n",
    "    if opt_parameters['function'] == 'Adam':\n",
    "        \n",
    "        optimizer = torch.optim.Adam(params_to_update, \n",
    "                                     lr = opt_parameters['lr'],\n",
    "                                     betas = opt_parameters['betas'],\n",
    "                                     eps = opt_parameters['eps'],\n",
    "                                     weight_decay = opt_parameters['weight_decay'],\n",
    "                                     amsgrad = opt_parameters['amsgrad']\n",
    "                                    )\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLossFunction(loss_nme):\n",
    "    \n",
    "    loss_parameters = LOSS_LIST[loss_nme]\n",
    "\n",
    "    if loss_parameters['function'] == 'SmoothL1Loss':\n",
    "        criterion = nn.SmoothL1Loss(\n",
    "            size_average = loss_parameters['size_average'],\n",
    "            reduce = loss_parameters['reduce'],\n",
    "            reduction = loss_parameters['reduction']\n",
    "        )\n",
    "\n",
    "    elif loss_parameters['function'] == 'CrossEntropyLoss':\n",
    "        criterion = nn.CrossEntropyLoss(\n",
    "            weight = loss_parameters['weight'],\n",
    "            size_average = loss_parameters['size_average'],\n",
    "            ignore_index = loss_parameters['ignore_index'],\n",
    "            reduce = loss_parameters['reduce'],\n",
    "            reduction = loss_parameters['reduction']\n",
    "        )\n",
    "\n",
    "    elif loss_parameters['function'] == 'NLLLoss':\n",
    "\n",
    "        criterion = nn.NLLLoss(\n",
    "            weight = loss_parameters['weight'],\n",
    "            size_average = loss_parameters['size_average'],\n",
    "            ignore_index = loss_parameters['ignore_index'],\n",
    "            reduce = loss_parameters['reduce'],\n",
    "            reduction = loss_parameters['reduction']\n",
    "        )\n",
    "\n",
    "    elif loss_parameters['function'] == 'QuadraticKappa':\n",
    "        criterion = QuadraticKappa(\n",
    "            n_classes = loss_parameters['n_classes']\n",
    "        )\n",
    "        \n",
    "    elif loss_parameters['function'] == 'WeightedMultiLabelLogLoss':\n",
    "\n",
    "        criterion = WeightedMultiLabelLogLoss(\n",
    "            n_classes = loss_parameters['n_classes'],\n",
    "            weight = loss_parameters['weight']\n",
    "        )\n",
    "        \n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def onehot(labels, num_classes):\n",
    "    return torch.zeros(len(labels), num_classes).scatter_(1, labels.unsqueeze(1).cpu(), 1.).cuda()\n",
    "\n",
    "def train_model(model, model_name, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_loss = 999\n",
    "    \n",
    "    print(model_name)\n",
    "    print('-' * 100)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        epoch_since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            #running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        \n",
    "                        outputs = torch.sigmoid(outputs)\n",
    "                        aux_outputs = torch.sigmoid(aux_outputs)\n",
    "                        \n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                        \n",
    "                    else:\n",
    "                        \n",
    "                        outputs = model(inputs)\n",
    "                        outputs = torch.sigmoid(outputs)\n",
    "                        \n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                    #_, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                #running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            #epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            #lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            \n",
    "            # Write loss into Tensorboard\n",
    "            tensorboard.add_scalar('Loss {}'.format(phase), epoch_loss, epoch)\n",
    "            #tensorboard.add_scalar('Acc {}'.format(phase), epoch_acc, epoch)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print('Saving the best model...')\n",
    "                best_loss = epoch_loss\n",
    "                torch.save(model, MODEL_DIR + '/' + model_name + '_imgsize' + str(INPUT_SIZE) + '_loss' + str(best_loss) + '.pt')\n",
    "            \n",
    "        epoch_time_elapsed = time.time() - epoch_since\n",
    "        print('Epoch time elapsed: {:.0f}m {:.0f}s'.format(epoch_time_elapsed // 60, epoch_time_elapsed % 60))\n",
    "            \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Loss: {:4f}'.format(best_loss))\n",
    "    print('')\n",
    "\n",
    "    # load best model weights\n",
    "    model = loadBestModel(model_name)\n",
    "    return model, best_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = ((output.cpu()).data).numpy()\n",
    "    def remove(self): self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCAM(feature_conv, weight_fc, class_idx):\n",
    "    _, nc, h, w = feature_conv.shape\n",
    "    cam = weight_fc[class_idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "    cam = cam.reshape(h, w)\n",
    "    cam = cam - np.min(cam)\n",
    "    cam_img = cam / np.max(cam)\n",
    "    return [cam_img]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = getModel('PreVGG16', NUM_CLASSES)\n",
    "model = resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(2048, NUM_CLASSES) \n",
    "\n",
    "model.eval()\n",
    "inputs, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = model._modules.get('layer4')\n",
    "activated_features = SaveFeatures(final_layer)\n",
    "prediction = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probabilities = F.softmax(prediction, 1).data.squeeze()\n",
    "activated_features.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_softmax_params = list(model._modules.get('fc').parameters())\n",
    "weight_softmax = np.squeeze(weight_softmax_params[0].cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idx = topk(pred_probabilities,1)[1].int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = getCAM(activated_features.features, weight_softmax, class_idx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "original_img = np.transpose(inputs[0].numpy(), (1,2,0))\n",
    "heat_map = skimage.transform.resize(overlay[0], original_img.shape[0:2])\n",
    "plt.imshow(original_img)\n",
    "plt.imshow(heat_map, alpha=0.5, cmap='jet');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = getModel('PreVGG16', NUM_CLASSES)\n",
    "model = resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(512, NUM_CLASSES) \n",
    "\n",
    "model.eval()\n",
    "inputs, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = model._modules.get('layer4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activated_features = SaveFeatures(final_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probabilities = F.softmax(prediction, 1).data.squeeze()\n",
    "activated_features.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import topk\n",
    "topk(pred_probabilities,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_softmax_params = list(model._modules.get('fc').parameters())\n",
    "weight_softmax = np.squeeze(weight_softmax_params[0].cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idx = topk(pred_probabilities,1)[1].int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = getCAM(activated_features.features, weight_softmax, class_idx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "original_img = np.transpose(inputs[0].numpy(), (1,2,0))\n",
    "heat_map = skimage.transform.resize(overlay[0], original_img.shape[0:2])\n",
    "plt.imshow(original_img)\n",
    "plt.imshow(heat_map, alpha=0.5, cmap='jet');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(heat_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(heat_map, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_name_list = []\n",
    "metric_list = []\n",
    "\n",
    "for m in MODELS:\n",
    "    for o in OPTIMIZERS:\n",
    "        for l in LOSSES:\n",
    "            \n",
    "            model = getModel(m, NUM_CLASSES)\n",
    "            \n",
    "            optimizer = getOptimizer(o, model)\n",
    "            \n",
    "            criterion = getLossFunction(l)\n",
    "            \n",
    "            model_name = f'{m}_{o}_{l}'\n",
    "            \n",
    "            tensorboard = SummaryWriter(comment = model_name)\n",
    "            \n",
    "            # Train and evaluate\n",
    "            model, best_metric = train_model(\n",
    "                model, \n",
    "                model_name, \n",
    "                dataloaders_dict, \n",
    "                criterion, \n",
    "                optimizer, \n",
    "                num_epochs=NUM_EPOCH, \n",
    "                is_inception=MODEL_LIST[m]['is_inception'])\n",
    "            \n",
    "            model_name_list.append(model_name)\n",
    "            metric_list.append(best_metric)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Best Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()    \n",
    "width = 0.75 # the width of the bars \n",
    "ind = np.arange(len(metric_list))  # the x locations for the groups\n",
    "ax.barh(ind, metric_list, width)\n",
    "ax.set_yticks(ind+width/2)\n",
    "ax.set_yticklabels(model_name_list, minor=False)\n",
    "plt.xlabel('Loss')\n",
    "for i, v in enumerate(metric_list):\n",
    "    ax.text(v, i, str(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_name_list)\n",
    "print(metric_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Janelamento 40-40. Melhor resultado:\n",
    "\n",
    "FineTuningResNet50_DefaultAdam_WeightedMultiLabelLogLoss_imgsize299_loss0.11020950659861853.pt\n",
    "FineTuningDensenet121_DefaultAdam_WeightedMultiLabelLogLoss_imgsize299_loss0.10916280491013167.pt\n",
    "\n",
    "* Janelamento 100-50. Melhor resultado:\n",
    "\n",
    "FineTuningResNet50_DefaultAdam_WeightedMultiLabelLogLoss_imgsize299_loss0.1011478987694739.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
