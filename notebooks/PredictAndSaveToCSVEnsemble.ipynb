{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.models import densenet121, vgg16, resnet50, inception_v3\n",
    "import glob\n",
    "from torch.autograd import Variable\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "from sklearn.utils import class_weight\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/srv/app/data'\n",
    "\n",
    "DATA_DIR = BASE_DIR + '/data'\n",
    "\n",
    "MODEL_DIR = BASE_DIR + '/models/'\n",
    "\n",
    "TEST_DIR = DATA_DIR + '/numpy_array/stage_2_test_images_299_roi_interpolated/'\n",
    "IMAGE_FORMAT = 'npy'\n",
    "\n",
    "BATCH_SIZE = 150\n",
    "\n",
    "LABEL_COLUMN = ['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
    "\n",
    "targets = ['ID', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural', 'any']\n",
    "\n",
    "files_list = os.listdir(TEST_DIR)\n",
    "\n",
    "files_ids = [x.split('.')[0] for x in files_list]\n",
    "\n",
    "CUDA_DEVICES = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "is_cuda=False\n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True\n",
    "print(is_cuda)    \n",
    "\n",
    "# Detect if we have a GPU available\n",
    "cuda_list = ','.join(str(c) for c in CUDA_DEVICES)\n",
    "device = torch.device(\"cuda:{}\".format(cuda_list) if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121232"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPredictDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, img_folder, img_ext='png', transform=None, index=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (dataframe): Dataframe with images ID.\n",
    "            img_folder (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.img_folder = img_folder\n",
    "        self.img_ext = img_ext\n",
    "        self.transform = transform\n",
    "        self.index = index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_folder, self.X.iloc[idx, 0] + '.' + self.img_ext)\n",
    "        #image = np.load(img_name).astype('uint8')\n",
    "        image = np.load(img_name)\n",
    "        \n",
    "        if self.index:\n",
    "            image = image[:,:,[int(self.index)]]\n",
    "            image = np.repeat(image, 3, axis=2)\n",
    "        if self.transform:\n",
    "        \n",
    "            image = self.transform(TF.to_pil_image(image))\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(files_ids, columns =['ID']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictProbas(model, transform, layer=None):\n",
    "#     torch.cuda.empty_cache()\n",
    "    dataset = CustomPredictDataset(\n",
    "                            X=X, \n",
    "                            img_folder=TEST_DIR, \n",
    "                            img_ext=IMAGE_FORMAT,\n",
    "                            transform=transform,\n",
    "                            index=layer\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    outputs = torch.zeros(1, 6).to(device)\n",
    "    with torch.no_grad():\n",
    "        for inputs in tqdm(loader):\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            if type(output) == tuple:\n",
    "                output = output[0]\n",
    "            output = torch.sigmoid(output)\n",
    "            outputs = torch.cat((outputs, output))\n",
    "    outputs = outputs[1:,:]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTimeAugmentationPredict(model, transform_list, layer=None):\n",
    "    model = torch.load(MODEL_DIR+model)\n",
    "    model.eval()\n",
    "    sumTensor = torch.zeros(len(X),6).to(device)\n",
    "    for transform in transform_list:\n",
    "        print('Transform {}'.format(str(transform)))\n",
    "        predict = predictProbas(model, transform, layer)\n",
    "        sumTensor = sumTensor + predict\n",
    "    finalPredict = sumTensor / len(transform_list)\n",
    "    return finalPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembleModelsTestTimeAugmentation(models_list, transform_list, layer=None):\n",
    "    sumTensor = torch.zeros(len(X),6).to(device)\n",
    "    for model in models_list:\n",
    "        print('Model: {}'.format(model))\n",
    "        predict = testTimeAugmentationPredict(model, transform_list, layer)\n",
    "        sumTensor = sumTensor + predict\n",
    "    finalPredict = sumTensor / len(models_list)\n",
    "    return finalPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transf = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transfA1 = transforms.Compose([\n",
    "    transforms.RandomRotation((0,360)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transfA2 = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transfA3 = transforms.Compose([\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transforms_list = [test_transf, test_transfA1, test_transfA2, test_transfA3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FineTuningDensenet121MultiTaskV2_SGDMomentumV7_WeightedMultiLabelLogLoss_imgsize299_loss0.06919282247931359.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/809 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Compose(\n",
      "    ToTensor()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 809/809 [10:58<00:00,  1.23it/s]\n",
      "  0%|          | 0/809 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Compose(\n",
      "    RandomRotation(degrees=(0, 360), resample=False, expand=False)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 809/809 [05:22<00:00,  2.51it/s]\n",
      "  0%|          | 0/809 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Compose(\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 809/809 [04:49<00:00,  2.80it/s]\n",
      "  0%|          | 0/809 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Compose(\n",
      "    RandomVerticalFlip(p=0.5)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 809/809 [04:47<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FineTuningResNet50AttentionMultiTaskV2_SGDMomentumV7_WeightedMultiLabelLogLoss_imgsize299_loss0.07118233637800009.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/809 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Compose(\n",
      "    ToTensor()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 809/809 [04:10<00:00,  3.24it/s]\n",
      "  0%|          | 0/809 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Compose(\n",
      "    RandomRotation(degrees=(0, 360), resample=False, expand=False)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 809/809 [04:45<00:00,  2.83it/s]\n",
      "  0%|          | 0/809 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Compose(\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 809/809 [04:14<00:00,  3.18it/s]\n",
      "  0%|          | 0/809 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Compose(\n",
      "    RandomVerticalFlip(p=0.5)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 809/809 [04:13<00:00,  3.20it/s]\n"
     ]
    }
   ],
   "source": [
    "models = ['FineTuningDensenet121MultiTaskV2_SGDMomentumV7_WeightedMultiLabelLogLoss_imgsize299_loss0.06919282247931359.pt',\n",
    "         'FineTuningResNet50AttentionMultiTaskV2_SGDMomentumV7_WeightedMultiLabelLogLoss_imgsize299_loss0.07118233637800009.pt']\n",
    "\n",
    "ensemble = ensembleModelsTestTimeAugmentation(models, transforms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.DataFrame(ensemble.tolist(), columns = LABEL_COLUMN)\n",
    "Y = Y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121232"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = X.merge(Y, left_index = True, right_index = True)\n",
    "pred = pred[targets]\n",
    "pred = pd.melt(pred, id_vars=['ID'], value_vars=targets[1:])\n",
    "pred['ID'] = pred['ID']+'_'+pred['variable']\n",
    "pred = pred.drop('variable', axis =1)\n",
    "pred.columns = ['ID', 'Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "727392"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred) #471270"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.to_csv(DATA_DIR + '/predicts/stage_2_pred01.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
