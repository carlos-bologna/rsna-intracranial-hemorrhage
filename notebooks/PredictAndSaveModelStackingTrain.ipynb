{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.models import densenet121, vgg16, resnet50, inception_v3\n",
    "import glob\n",
    "from torch.autograd import Variable\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "from sklearn.utils import class_weight\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from dask.distributed import Client\n",
    "from dask import array as da\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/srv/app/data'\n",
    "\n",
    "DATA_DIR = BASE_DIR + '/data'\n",
    "\n",
    "MODEL_DIR = BASE_DIR + '/models/'\n",
    "\n",
    "TRAIN_DIR = DATA_DIR + '/numpy_array/stage_2_train_images_299_roi_interpolated/'\n",
    "IMAGE_FORMAT = 'npy'\n",
    "\n",
    "BATCH_SIZE = 168\n",
    "\n",
    "LABEL_COLUMN = ['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
    "\n",
    "targets = ['ID', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural', 'any']\n",
    "\n",
    "TRAIN_LABELS = DATA_DIR + '/stage_1_train_pivoted_z.csv'\n",
    "\n",
    "CUDA_DEVICES = [1,2,3]\n",
    "\n",
    "BLACK_LIST_ID = ['ID_6431af929', 'ID_8da38f2e4', 'ID_0e21abf7a', 'ID_470e639ae', 'ID_d91d52bdc', \n",
    "                 'ID_dfcb69305', 'ID_5005bcb25']\n",
    "\n",
    "files_list = os.listdir(TRAIN_DIR)\n",
    "\n",
    "files_ids = [x.split('.')[0] for x in files_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "is_cuda=False\n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True\n",
    "print(is_cuda)    \n",
    "\n",
    "# Detect if we have a GPU available\n",
    "cuda_list = ','.join(str(c) for c in CUDA_DEVICES)\n",
    "device = torch.device(\"cuda:{}\".format(cuda_list) if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPredictDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, img_folder, img_ext='png', transform=None, index=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (dataframe): Dataframe with images ID.\n",
    "            img_folder (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.img_folder = img_folder\n",
    "        self.img_ext = img_ext\n",
    "        self.transform = transform\n",
    "        self.index = index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_folder, self.X.iloc[idx].ID + '.' + self.img_ext)\n",
    "        #image = np.load(img_name).astype('uint8')\n",
    "        image = np.load(img_name)\n",
    "        \n",
    "        if self.index:\n",
    "            image = image[:,:,[int(self.index)]]\n",
    "            image = np.repeat(image, 3, axis=2)\n",
    "        if self.transform:\n",
    "        \n",
    "            image = self.transform(TF.to_pil_image(image))\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(files_ids, columns =['ID']) \n",
    "X = X.loc[~X.ID.isin(BLACK_LIST_ID)]\n",
    "\n",
    "X_stack = pd.DataFrame(files_ids, columns =['ID']) \n",
    "X_stack = X_stack.loc[~X_stack.ID.isin(BLACK_LIST_ID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "752796"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictProbas(model, model_name, transform, layer=None):\n",
    "    global X_stack\n",
    "    dataset = CustomPredictDataset(\n",
    "                            X=X, \n",
    "                            img_folder=TRAIN_DIR, \n",
    "                            img_ext=IMAGE_FORMAT,\n",
    "                            transform=transform[1],\n",
    "                            index=layer\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    outputs = torch.zeros(1, 6).to(device)\n",
    "    with torch.no_grad():\n",
    "        for inputs in tqdm(loader):\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            if type(output) == tuple:\n",
    "                output = output[0]\n",
    "            output = torch.sigmoid(output)\n",
    "            outputs = torch.cat((outputs, output))\n",
    "    outputs = outputs[1:,:]\n",
    "    labels = [item+\"_\"+model_name+transform[0] for item in LABEL_COLUMN]\n",
    "    Y_pred = pd.DataFrame(outputs.tolist(), columns = labels)\n",
    "    Y_pred = Y_pred.reset_index(drop=True)\n",
    "    X_stack = X_stack.merge(Y_pred, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTimeAugmentationPredict(model, transform_list, layer=None):\n",
    "    loaded_model = torch.load(MODEL_DIR+model[1])\n",
    "    loaded_model.eval()\n",
    "    for transform in transform_list:\n",
    "        print('Transform {}'.format(str(transform)))\n",
    "        predict = predictProbas(loaded_model, model[0],transform, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stackModelsTestTimeAugmentation(models_list, transform_list, layer=None):\n",
    "    for model in models_list:\n",
    "        print('Model: {}'.format(model[1]))\n",
    "        predict = testTimeAugmentationPredict(model, transform_list, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transf = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transfA1 = transforms.Compose([\n",
    "    transforms.RandomRotation((0,360)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transfA2 = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transfA3 = transforms.Compose([\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transforms_list = [('T1', test_transf), ('T2', test_transfA1), ('T3', test_transfA2), ('T4', test_transfA3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FineTuningResNet50AttentionMultiTaskV2_SGDMomentumV7_WeightedMultiLabelLogLoss_imgsize299_loss0.07118233637800009.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4481 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform ('T1', Compose(\n",
      "    ToTensor()\n",
      "))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4481/4481 [54:23<00:00,  1.37it/s]\n",
      "  0%|          | 0/4481 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform ('T2', Compose(\n",
      "    RandomRotation(degrees=(0, 360), resample=False, expand=False)\n",
      "    ToTensor()\n",
      "))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4481/4481 [56:49<00:00,  1.31it/s]\n",
      "  0%|          | 0/4481 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform ('T3', Compose(\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ToTensor()\n",
      "))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4481/4481 [54:38<00:00,  1.37it/s] \n",
      "  0%|          | 0/4481 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform ('T4', Compose(\n",
      "    RandomVerticalFlip(p=0.5)\n",
      "    ToTensor()\n",
      "))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4481/4481 [43:31<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FineTuningDensenet121MultiTaskV2_SGDMomentumV7_WeightedMultiLabelLogLoss_imgsize299_loss0.06919282247931359.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4481 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform ('T1', Compose(\n",
      "    ToTensor()\n",
      "))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4481/4481 [44:54<00:00,  1.66it/s]\n",
      "  0%|          | 0/4481 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform ('T2', Compose(\n",
      "    RandomRotation(degrees=(0, 360), resample=False, expand=False)\n",
      "    ToTensor()\n",
      "))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4481/4481 [1:04:36<00:00,  1.16it/s]\n",
      "  0%|          | 0/4481 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform ('T3', Compose(\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ToTensor()\n",
      "))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 4058/4481 [1:13:55<47:31,  6.74s/it]"
     ]
    }
   ],
   "source": [
    "models = [('ResNet','FineTuningResNet50AttentionMultiTaskV2_SGDMomentumV7_WeightedMultiLabelLogLoss_imgsize299_loss0.07118233637800009.pt'),\n",
    "         ('DenseNet','FineTuningDensenet121MultiTaskV2_SGDMomentumV7_WeightedMultiLabelLogLoss_imgsize299_loss0.06919282247931359.pt')]\n",
    "\n",
    "stackModelsTestTimeAugmentation(models, transforms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_stack) #752796"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stack.to_csv(DATA_DIR + '/predicts/stage_2_train_pred02.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_stacked = X_stack.drop('ID', axis = 1)\n",
    "Y = X_stack.merge(data, left_index = True, right_index = True)\n",
    "Y = Y[['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']]\n",
    "Y_labels = Y.to_numpy()\n",
    "Y_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=1, n_jobs = -1, n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_stacked, Y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "X_train_empty = X_stack[0:0]\n",
    "joblib.dump({'rf':rf,'X_train_empty':X_train_empty}, BASE_DIR + '/models/stackingRF.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
